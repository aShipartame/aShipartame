---
---

@UNPUBLISHED{FCAM,
  author={Tang, Shi and Chu, Chaoqun and Luo, Guiming and Ye, Xinchen and Xia, Zhiyi and Li, Haojie},
  title={Unleash the Power of Local Representations: Feature Calibration and Adaptive Metric for Few-Shot Learning},
  year={2023},
  note={Under review},
  Xabstract={Generalizing to novel classes unseen during training is a key challenge of few-shot classification. Recent metric-based methods try to address this by local representations. However, they are unable to take full advantage of them due to (i) improper supervision for pretraining the feature extractor, and (ii) lack of adaptability in the metric for handling various possible compositions of local feature sets. In this work, we unleash the power of local representations in improving novel-class generalization. For the feature extractor, we design a novel pretraining paradigm that learns randomly cropped patches by soft labels. It utilizes the class-level diversity of patches while diminishing the impact of their semantic misalignments to hard labels. To align network output with soft labels, we also propose a UniCon KL-Divergence that emphasizes the equal contribution of each base class in describing ``non-base'' patches. For the metric, we formulate measuring local feature sets as an entropy-regularized optimal transport problem to introduce the ability to handle sets consisting of homogeneous elements. Furthermore, we design a Modulate Module to endow the metric with the necessary adaptability. Our method achieves new state-of-the-art performance on three popular benchmarks. Moreover, it exceeds state-of-the-art transductive and cross-modal methods in the fine-grained scenario.},
  abstract={Generalizing to novel classes unseen during training is a key challenge of few-shot classification. Recent metric-based methods try to address this by local representations. However, they are unable to take full advantage of them due to (i) biased features caused by inheriting the old paradigm of using random cropping for augmentation, and (ii) a non-adaptive metric that cannot handle various possible compositions of local feature sets. In this work, we unleash the power of local representations in improving novel-class generalization. For the encoder, we design a novel pretraining paradigm that learns cropped patches by soft labels. It avoids the semantic misalignment between hard labels and patches while fully utilizing the class-level diversity of patches. To align network output with soft labels, we also propose a Smoothed KL-Divergence that emphasizes the equal contribution of each base class in describing “non-base” patches. For the metric, we propose to predict the adjustment coefficient of an introduced entropic term to endow optimal transport distances with the necessary adaptability. Our method achieves new state-of-the-art performance on three popular benchmarks. Moreover, it exceeds state-of-the-art transductive and cross-modal methods in the fine-grained scenario, revealing how much the poor local representations can degrade the performance in this scenario.},
  pdf={FCAM.pdf},
  Xsupp={FCAM_supp.pdf},
  preview={FCAM.png},
  selected={true}}

@INPROCEEDINGS{cross_modality,
  author={Tang, Shi and Ye, Xinchen and Xue, Fei and Xu, Rui},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Cross-Modality Depth Estimation via Unsupervised Stereo RGB-to-Infrared Translation}, 
  year={2023},
  abstract={Existing depth estimation methods infer scene depth only from stereo visible light (RGB) images. Since RGB imaging is sensitive to changes in light, it is difficult to estimate depth information accurately in some degraded visibility conditions. In contrast, infrared (IR) imaging captures thermal radiation and is not affected by brightness changing, providing extra clues for depth estimation. However, most datasets used for training in depth estimation do not have IR images paired with RGB-D data. Therefore, how to obtain the paired IR images and exploit the respective advantages of RGB and IR images to improve the performance of depth estimation, is of vital importance. Our core idea is to first develop an unsupervised RGB-to-IR translation (RIT) network with proposed Fourier domain adaptation and multi-space warping regularization to synthesize stereo IR images from their corresponding stereo RGB images. And then modified depth estimation backbones can be used as the cross-modality depth estimation (CDE) network to infer disparity maps from cross-modal RGB-IR stereo pairs. Assisted by the synthetic stereo IR images, we obtain superior performance just by flexibly deploying our framework to several off-the-shelf depth estimation backbones of single-modality (RGB) based methods.},
  html={https://ieeexplore.ieee.org/abstract/document/10095982},
  pdf={cross_modality.pdf},
  preview={cross_modality.png},
  selected={true}}
