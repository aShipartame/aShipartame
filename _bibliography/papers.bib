---
---

@UNPUBLISHED{FCAM,
  author={Tang, Shi and Chu, Chaoqun and Luo, Guiming and Ye, Xinchen and Xia, Zhiyi and Li, Haojie},
  title={Unleash the Power of Local Representations: Feature Calibration and Adaptive Metric for Few-Shot Learning},
  year={2023},
  note={Under review},
  Xabstract={Recent metric-based few-shot learning (FSL) methods tend to adopt a set of local features instead of a global embedding to represent an instance, bridging base and novel class samples through potential common local features to improve novel-class generalization. However, due to biased features caused by treating local patches as base class samples during pretraining and a non-adaptive metric that cannot handle various local feature sets, existing methods are unable to take full advantage of local representations, leading to insufficient improvement in novel-class generalization. To address these issues, we investigate Feature Calibration (FC) and an Adaptive Metric (AM) to propose a novel method for FSL, namely FCAM. We treat local patches as ``pseudo'' novel class samples and generate soft labels capable of describing them to calibrate the biased features while fully exploiting their potential in improving novel-class generalization. Meanwhile, we employ the dual-Sinkhorn Divergence (dSD) with a designed Regulation Module (RM) to endow the metric with the flexibility to handle various local feature sets. Our method achieves new state-of-the-art on three popular benchmarks. Moreover, it exceeds state-of-the-art transductive and cross-modal methods in the fine-grained scenario.},
  abstract={Generalizing to novel classes unseen during training is a key challenge of few-shot classification. Recent metric-based methods try to address this by local representations. However, they are unable to take full advantage of them due to (i) biased features caused by inheriting the old paradigm of using random cropping for augmentation, and (ii) a non-adaptive metric that cannot handle various possible compositions of local feature sets. In this work, we unleash the power of local representations in improving novel-class generalization. For the encoder, we design a novel pretraining paradigm that learns cropped patches by soft labels. It avoids the semantic misalignment between hard labels and patches while fully utilizing the class-level diversity of patches. To align network output with soft labels, we also propose a Smoothed KL-Divergence that emphasizes the equal contribution of each base class in describing “non-base” patches. For the metric, we propose to predict the adjustment coefficient of an introduced entropic term to endow optimal transport distances with the necessary adaptability. Our method achieves new state-of-the-art performance on three popular benchmarks. Moreover, it exceeds state-of-the-art transductive and cross-modal methods in the fine-grained scenario, revealing how much the poor local representations can degrade the performance in this scenario.}
  pdf={FCAM.pdf},
  Xsupp={FCAM_supp.pdf},
  preview={FCAM.png},
  selected={true}}

@INPROCEEDINGS{cross_modality,
  author={Tang, Shi and Ye, Xinchen and Xue, Fei and Xu, Rui},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Cross-Modality Depth Estimation via Unsupervised Stereo RGB-to-Infrared Translation}, 
  year={2023},
  abstract={Existing depth estimation methods infer scene depth only from stereo visible light (RGB) images. Since RGB imaging is sensitive to changes in light, it is difficult to estimate depth information accurately in some degraded visibility conditions. In contrast, infrared (IR) imaging captures thermal radiation and is not affected by brightness changing, providing extra clues for depth estimation. However, most datasets used for training in depth estimation do not have IR images paired with RGB-D data. Therefore, how to obtain the paired IR images and exploit the respective advantages of RGB and IR images to improve the performance of depth estimation, is of vital importance. Our core idea is to first develop an unsupervised RGB-to-IR translation (RIT) network with proposed Fourier domain adaptation and multi-space warping regularization to synthesize stereo IR images from their corresponding stereo RGB images. And then modified depth estimation backbones can be used as the cross-modality depth estimation (CDE) network to infer disparity maps from cross-modal RGB-IR stereo pairs. Assisted by the synthetic stereo IR images, we obtain superior performance just by flexibly deploying our framework to several off-the-shelf depth estimation backbones of single-modality (RGB) based methods.},
  html={https://ieeexplore.ieee.org/abstract/document/10095982},
  pdf={cross_modality.pdf},
  preview={cross_modality.png},
  selected={true}}
