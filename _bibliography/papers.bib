---
---

@UNPUBLISHED{UFAM,
  author={Tang, Shi and Chu, Chaoqun and Luo, Guiming and Ye, Xinchen and Xia, Zhiyi and Li, Haojie},
  title={Unleash the Power of Local Representations: Unbiased Features and Adaptive Metric for Few-Shot Learning},
  year={2023},
  abstract={Recent metric-based few-shot learning (FSL) methods tend to adopt a set of local features instead of a global embedding to represent an instance, bridging base and novel class samples through potential common local features to improve novel-class generalization. However, due to biased features caused by treating local patches as base class samples during pretraining and a non-adaptive metric that cannot handle various local feature sets, existing methods are unable to take full advantage of local representations, leading to insufficient improvement of novel-class generalization. To address these issues, we investigate unbiased features (UF) and an adaptive metric (AM) to propose a novel method for FSL, namely UFAM. We treat local patches as "pseudo" novel class samples and generate soft labels capable of describing them to calibrate the biased features while fully exploiting their potential in improving novel-class generalization. Meanwhile, we employ the dual-Sinkhorn Divergence (dSD) with a designed Regulation Module (RM) to endow the metric with the flexibility to handle various local feature sets, realizing an adaptive metric. Our method achieves new state-of-the-art on three popular benchmarks. Moreover, it exceeds state-of-the-art transductive and cross-modal methods in the fine-grained scenario.},
  selected={true}}

@INPROCEEDINGS{cross_modality,
  author={Tang, Shi and Ye, Xinchen and Xue, Fei and Xu, Rui},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Cross-Modality Depth Estimation via Unsupervised Stereo RGB-to-Infrared Translation}, 
  year={2023},
  abstract={Existing depth estimation methods infer scene depth only from stereo visible light (RGB) images. Since RGB imaging is sensitive to changes in light, it is difficult to estimate depth information accurately in some degraded visibility conditions. In contrast, infrared (IR) imaging captures thermal radiation and is not affected by brightness changing, providing extra clues for depth estimation. However, most datasets used for training in depth estimation do not have IR images paired with RGB-D data. Therefore, how to obtain the paired IR images and exploit the respective advantages of RGB and IR images to improve the performance of depth estimation, is of vital importance. Our core idea is to first develop an unsupervised RGB-to-IR translation (RIT) network with proposed Fourier domain adaptation and multi-space warping regularization to synthesize stereo IR images from their corresponding stereo RGB images. And then modified depth estimation backbones can be used as the cross-modality depth estimation (CDE) network to infer disparity maps from cross-modal RGB-IR stereo pairs. Assisted by the synthetic stereo IR images, we obtain superior performance just by flexibly deploying our framework to several off-the-shelf depth estimation backbones of single-modality (RGB) based methods.},
  html={https://ieeexplore.ieee.org/abstract/document/10095982},
  pdf={cross_modality.pdf},
  preview={cross_modality.png},
  selected={true}}
